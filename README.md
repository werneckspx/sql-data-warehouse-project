# Data Warehouse e Analytics Project

Este projeto utiliza uma arquitetura de Data Warehouse moderna baseada na Arquitetura Medallion, que organiza os dados em camadas (Bronze, Silver e Gold), com o objetivo de garantir a ingestÃ£o eficiente de dados, padronizaÃ§Ã£o, escabilidade e disponibilizaÃ§Ã£o para consumo analÃ­tico e preditivo.

### ğŸ”§ Componentes Principais

* **ğŸ“ Arquitetura de Dados:** Estruturada com base na abordagem Medallion, permitindo um fluxo limpo e controlado de dados desde o estÃ¡gio bruto atÃ© a camada pronta para consumo.
* **âš™ï¸ Pipelines de ETL:** AutomatizaÃ§Ã£o do processo de extraÃ§Ã£o, transformaÃ§Ã£o e carga dos dados provenientes de sistemas-fonte (como CRM e ERP) para o Data Warehouse.
* **ğŸ§© Modelagem de Dados:** CriaÃ§Ã£o de **tabelas fato e dimensÃ£o**, otimizadas para consultas analÃ­ticas de alta performance.
* **ğŸ“Š AnÃ¡lises e RelatÃ³rios:** GeraÃ§Ã£o de insights por meio de consultas SQL, apoiando decisÃµes orientadas a dados.

> âœ… Desenvolvido com foco em qualidade de dados, arquitetura modular e prontidÃ£o para uso em BI e Machine Learning.

---

### ğŸ“ 1. Data Sources

Os dados sÃ£o provenientes de sistemas transacionais como CRM e ERP, exportados no formato **CSV** e armazenados em **pastas locais**.

* **Formato:** Arquivos CSV
* **Interface de ingestÃ£o:** Leitura de arquivos em diretÃ³rios/folders

---

### ğŸŸ« 2. Bronze Layer â€“ Raw Data

Nesta camada, os dados sÃ£o ingeridos em seu formato original, sem qualquer tipo de transformaÃ§Ã£o. Ela serve como repositÃ³rio histÃ³rico bruto para rastreabilidade.

* **Tipo de Objeto:** Tabelas
* **Carga:** Batch Processing, Full Load, Truncate & Insert
* **TransformaÃ§Ãµes:** Nenhuma (dados "as-is")
* **Modelo de Dados:** NÃ£o estruturado
* **ExecuÃ§Ã£o:** Procedures SQL automatizadas

---

### â¬œ 3. Silver Layer â€“ Cleaned & Standardized Data

Aqui os dados passam por processos de limpeza, padronizaÃ§Ã£o e enriquecimento, tornando-se consistentes e prontos para anÃ¡lises intermediÃ¡rias.

* **Tipo de Objeto:** Tabelas
* **Carga:** Batch Processing, Full Load, Truncate & Insert
* **TransformaÃ§Ãµes Aplicadas:**

  * Data Cleansing (remoÃ§Ã£o de duplicatas, nulos, erros)
  * Data Standardization (formataÃ§Ã£o de datas, nÃºmeros, etc.)
  * Data Normalization (ajuste de escalas ou padrÃµes)
  * Derived Columns (criaÃ§Ã£o de colunas)
  * Data Enrichment (inclusÃ£o de informaÃ§Ãµes complementares)
* **Modelo de Dados:** Ainda nÃ£o modelado (intermediÃ¡rio)
* **ExecuÃ§Ã£o:** Procedures SQL

---

### ğŸŸ¨ 4. Gold Layer â€“ Business-Ready Data

Camada otimizada para o consumo. Os dados sÃ£o integrados e modelados segundo lÃ³gicas de negÃ³cio, prontos para anÃ¡lises em ferramentas de BI e modelos de machine learning.

* **Tipo de Objeto:** Views (consultas SQL prÃ©-definidas)
* **Carga:** Nenhuma (execuÃ§Ã£o sob demanda via views)
* **TransformaÃ§Ãµes Aplicadas:**

  * Data Integration (junÃ§Ãµes)
  * Aggregations 
  * Business Logic 
* **Modelo de Dados:**

  * Star Schema
  * Flat Tables
  * Aggregated Tables

---

### ğŸ“Š 5. Consumption Layer â€“ Data Consumers

Os dados da Gold Layer podem ser consumidos por diferentes aplicaÃ§Ãµes e usuÃ¡rios finais:

* **BI & Dashboards:** Power BI, Tableau, etc.
* **Consultas Ad-Hoc:** SQL queries diretas em views
* **Machine Learning:** Dados prontos para pipelines preditivos

---

### ğŸ¯ BenefÃ­cios da Arquitetura

* SeparaÃ§Ã£o clara de responsabilidades por camada
* Rastreabilidade e versionamento de dados
* Flexibilidade para diferentes tipos de consumo (BI, ML, SQL)
* AderÃªncia a boas prÃ¡ticas de governanÃ§a e qualidade de dados

---

## ğŸš€ Project Requirements

Para executar e testar este projeto de Data Warehouse, sÃ£o necessÃ¡rios os seguintes requisitos:

### Banco de Dados

- **PostgreSQL** (utilizado 17)
  - ExtensÃµes padrÃ£o habilitadas (ex: `plpgsql`)

### Ferramentas

- **psql** (cliente de linha de comando do PostgreSQL)  
  Ou qualquer ferramenta de administraÃ§Ã£o SQL compatÃ­vel (utilizado pgAdmin).

### Estrutura de DiretÃ³rios

```
sql-data-warehouse-project/
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ source_crm/
â”‚   â”‚   â”œâ”€â”€ cust_info.csv
â”‚   â”‚   â”œâ”€â”€ prd_info.csv
â”‚   â”‚   â””â”€â”€ sales_details.csv
â”‚   â””â”€â”€ source_erp/
â”‚       â”œâ”€â”€ cust_az12.csv
â”‚       â”œâ”€â”€ loc_a101.csv
â”‚       â””â”€â”€ px_cat_g1v2.csv
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_database_sql
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ ddl_bronze.sql
â”‚   â”‚   â””â”€â”€ proc_load_bronze.sql
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ ddl_silver.sql
â”‚   â”‚   â””â”€â”€ proc_load_silver.sql
â”‚   â””â”€â”€ gold/
â”‚       â””â”€â”€ ddl_gold.sql
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ quality_silver_checks.sql
â”‚   â””â”€â”€ quality_gold_checks.sql
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

- **datasets/**: ContÃ©m os arquivos CSV de entrada, separados por origem (CRM e ERP).
- **scripts/**: Scripts SQL para criaÃ§Ã£o de banco, schemas, tabelas e procedures de carga e transformaÃ§Ã£o.
  - **bronze/**: Scripts para camada Bronze (dados brutos).
  - **silver/**: Scripts para camada Silver (dados tratados).
  - **gold/**: Scripts para camada Gold (views analÃ­ticas).
- **test/**: Scripts de checagem e validaÃ§Ã£o da qualidade dos dados.
- **README.md**: DocumentaÃ§Ã£o do projeto.
- **.gitignore**: Arquivos e pastas ignorados pelo controle de versÃ£o.

---

## ğŸ‘¤ Sobre Mim

OlÃ¡! Meu nome Ã© Felipe Werneck e sou um estudante de **engenharia da computaÃ§Ã£o** e tenho interesse na Ã¡rea de engenharia e anÃ¡lise de dados;

Desenvolvi este projeto como parte do meu portfÃ³lio com o objetivo de:

* Aprimorar habilidades prÃ¡ticas em **modelagem de dados**, **SQL** e **data warehouse**;
* Demonstrar o uso da **Arquitetura Medallion** em um contexto realista;
* Praticar boas prÃ¡ticas de versionamento, organizaÃ§Ã£o e documentaÃ§Ã£o de projetos de dados.

ğŸ“« **Contato**

* [LinkedIn](www.linkedin.com/in/felipe-werneck-93520a209)
* [E-mail](mailto:felipwerneck@gmail.com)

AgradeÃ§o pela visita ao projeto! ğŸ˜Š

> ğŸ” Este projeto foi desenvolvido com base em um estudo guiado pelo tutorial de [Data with Baraa](https://www.youtube.com/@DataWithBaraa).
